# -*- coding: utf-8 -*-
"""implementation_with_GV3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X1NLKS8j55Fnca7zTx88Jp3_tzl2au42

<h3>Step 1: Import libraries</h3>
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
import math
import warnings
warnings.filterwarnings('ignore')

"""<h3>Step 2: Import data</h3>"""

# Read dataset
train_data_path = os.path.abspath('train_data2 orr.csv')
test_data_path = os.path.abspath('testing_data2 orr.csv')

# merging two csv files
dataframe = pd.concat(map(pd.read_csv, [train_data_path, test_data_path]), ignore_index=True)

dataframe.head()

"""<h3>Step 3: Data Analysis / Preprocessing</h3>

EDA typically involves several key steps, including:

- Data cleaning and preparation involve removing missing or incorrect values, transforming variables, and handling outliers.
- Data visualization is the process of creating graphs, charts, and other visual representations of the data to help identify patterns, relationships, and anomalies.
- Statistical analysis involves applying mathematical and statistical methods to the data to identify important features and relationships.

<b>Get required data</b>

We don’t need the Filename column so we can drop it.
"""

# Get required data
dataframe.drop(columns = ['Filename'], inplace=True)
#dataframe.drop(columns = ['Unnamed: 14'], inplace=True)
dataframe.head()

"""<b>Describe data</b>

Get statistical description of data using Pandas describe() function. It shows us the count, mean, standard deviation, and range of data.
"""

# Describe data
dataframe.describe()

dataframe.info()

"""<b>Distribution of data</b>

Check data distribution.
"""

# distribution
sns.displot(dataframe['PM2.5'])
sns.displot(dataframe['PM10'])
sns.displot(dataframe['O3'])
sns.displot(dataframe['CO'])
sns.displot(dataframe['SO2'])
sns.displot(dataframe['NO2'])

"""<b>Label encoding</b>

Label encoding is a preprocessing technique in machine learning and data analysis where categorical data is converted into numerical values, to make it compatible with mathematical operations and models.
"""

# Label encoding
le = LabelEncoder()
dataframe['Location'] = le.fit_transform(dataframe['Location'])
dataframe['Hour'] = le.fit_transform(dataframe['Hour'])
#dataframe['AQI_Class'] = le.fit_transform(dataframe['AQI_Class'])

dataframe.head()

"""<b>Correlation matrix</b>

A correlation matrix is a table that summarizes the relationship between multiple variables in a dataset. It shows the correlation coefficients between each pair of variables, which indicate the strength and direction of the relationship between the variables. It is useful for identifying highly correlated variables and selecting a subset of variables for further analysis.

The correlation coefficient can range from -1 to 1, where:

- A correlation coefficient of -1 indicates a strong negative relationship between two variables
- A correlation coefficient of 0 indicates no relationship between two variables
- A correlation coefficient of 1 indicates a strong positive relationship between two variables
"""

# Correlation matrix
#dataframe.corr()
#sns.heatmap(dataframe.corr())

'''
dataframe['z_score'] = (dataframe['CO'] - dataframe['CO'].mean()) / dataframe['CO'].std()

# Отбор выбросов (по правилу: Z-оценка > 3)
outliers = dataframe[np.abs(dataframe['z_score']) > 3]
print("Выбросы по Z-оценке:\n", outliers)

# Визуализация
plt.figure(figsize=(8, 4))
sns.scatterplot(x=range(len(dataframe)), y=dataframe['CO'], color='blue', label="Данные")
sns.scatterplot(x=outliers.index, y=outliers['CO'], color='red', label="Выбросы")
plt.title("Выбросы, выявленные с помощью Z-оценки")
plt.legend()
plt.show()
'''

"""<b>Drop insignificant data</b>

From the correlation matrix, we see that Year and Month is not correlated to other attributes so we can drop that too.
"""

# Drop Year and Month columns
dataframe.drop(columns=['Year'], inplace=True)
dataframe.drop(columns=['Month'], inplace=True)

# Review missing data
print(dataframe.isnull().sum())

# Eliminate rows with at least three missing values
dataframe = dataframe.dropna(thresh=3)
dataframe.head()

# Review rows with remaining missing values
missing_O3_indices = [i for i,v in enumerate(dataframe['O3'].isnull()) if v==True]
missing_CO_indices = [i for i,v in enumerate(dataframe['CO'].isnull()) if v==True]
missing_SO2_indices = [i for i,v in enumerate(dataframe['SO2'].isnull()) if v==True]
missing_NO2_indices = [i for i,v in enumerate(dataframe['NO2'].isnull()) if v==True]
all_missing_indices = list(set(missing_O3_indices + missing_CO_indices + missing_SO2_indices + missing_NO2_indices))
print("Number of missing SO2 data is", len(all_missing_indices))
missingData = dataframe.iloc[all_missing_indices]

# Inpute missing values for SO2 based on the mean of similar rows
for index, row in missingData.iterrows():
    o3 = row[6]
    co = row[7]
    so2 = row[8]
    no2 = row[9]
    if math.isnan(row[6]): dataframe.at[index, 'O3'] = dataframe.loc[:, 'O3'].mean()
    if math.isnan(row[7]): dataframe.at[index, 'CO'] = dataframe.loc[:, 'CO'].mean()
    if math.isnan(row[8]): dataframe.at[index, 'SO2'] = dataframe.loc[:, 'SO2'].mean()
    if math.isnan(row[9]): dataframe.at[index, 'NO2'] = dataframe.loc[:, 'NO2'].mean()

# Review missing data
print(dataframe.isnull().sum())

plt.figure(figsize=(13, 4))
sns.boxplot(data=dataframe, x="AQI_Class", y="NO2")
plt.title("NO2 выбросы")
plt.show()
plt.figure(figsize=(13, 4))
sns.boxplot(data=dataframe, x="AQI_Class", y="PM2.5")
plt.title("PM2.5 выбросы")
plt.show()
plt.figure(figsize=(13, 4))
sns.boxplot(data=dataframe, x="AQI_Class", y="PM10")
plt.title("PM10 выбросы")
plt.show()
plt.figure(figsize=(13, 4))
sns.boxplot(data=dataframe, x="AQI_Class", y="O3")
plt.title("O3 выбросы")
plt.show()
plt.figure(figsize=(13, 4))
sns.boxplot(data=dataframe, x="AQI_Class", y="CO")
plt.title("CO выбросы")
plt.show()
plt.figure(figsize=(13, 4))
sns.boxplot(data=dataframe, x="AQI_Class", y="SO2")
plt.title("SO2 выбросы")
plt.show()

sns.pairplot(dataframe, hue="AQI_Class")
plt.show()

"""<h3>Step 4: Split data</h3>

Splitting data into independent and dependent variables involves separating the input features (independent variables) from the target variable (dependent variable). The independent variables are used to predict the value of the dependent variable.

The data is then split into a training set and a test set, with the training set used to fit the model and the test set used to evaluate its performance.

<b>Independent / Dependent variables</b>
"""

# Split data into dependent/independent variables
X = dataframe.iloc[:, :-1].values
y = dataframe.iloc[:, -1].values

# encoding categorical data e.g. gender as a dummy variable
labelencoder_X = LabelEncoder()
X[:,1] = labelencoder_X.fit_transform(X[:,1])

# encoding categorical data e.g. disease outcome as a dummy variable
y,class_names = pd.factorize(y)

print(class_names)

"""<b>Train / Test split</b>

The data is usually divided into two parts, with the majority of the data used for training the model and a smaller portion used for testing. We have split the data into 75% for training and 25% for testing.
"""

# Splitting the dataset into the Training set and Test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state = 42)

"""<h3>Step 5: Feature scaling</h3>

Feature scaling is a method of transforming the values of numeric variables so that they have a common scale as machine learning algorithms are sensitive to the scale of the input features.

There are two common methods of feature scaling: normalization and standardization.

- Normalization scales the values of the variables so that they fall between 0 and 1. This is done by subtracting the minimum value of the feature and dividing it by the range (max-min).
- Standardization transforms the values of the variables so that they have a mean of 0 and a standard deviation of 1. This is done by subtracting the mean and dividing it by the standard deviation.

Feature scaling is usually performed before training a model, as it can improve the performance of the model and reduce the time required to train it, and helps to ensure that the algorithm is not biased towards variables with larger values.
"""

# Scale dataset
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""**General Tree models**"""

classifier = DecisionTreeClassifier(criterion = 'entropy', max_depth=2, min_samples_split=50, min_samples_leaf=20, random_state=42)
classifier.fit(X_train, y_train)

y_pred_train = classifier.predict(X_train)
print(X_train)
print(y_pred_train)
y_pred_test = classifier.predict(X_test)
print(X_test)
print(y_pred_test)
accuracy = metrics.accuracy_score(y_test, y_pred_test)
print("Accuracy: {:.2f}".format(accuracy))
# Classification report
print(y_train)
print(y_pred_train)

print(classification_report(y_train, y_pred_train, target_names=class_names))
print("F1 Score: ", f1_score(y_train, y_pred_train, average='micro'))

cf_matrix = confusion_matrix(y_train, y_pred_train)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

"""**GRID Search for Tree**"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, f1_score

param_grid = {
    'criterion': ['gini', 'entropy', 'log_loss'],
    'splitter': ['best', 'random'],
    'max_depth': list(range(1, 5)),
    'min_samples_split': list(range(1, 5)),
    'min_samples_leaf': list(range(1, 5)),
    'max_features': [None, 'sqrt', 'log2'],
    'max_leaf_nodes': [None, 10, 20, 50, 100],
    'min_impurity_decrease': [0.0, 0.01, 0.02, 0.05, 0.1]
}

# Desicion tree model
dt = DecisionTreeClassifier(random_state=42)

# Grid Search Implementation
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_
print(f"Best params: {grid_search.best_params_}")

# Prediction
y_pred = best_model.predict(X_test)

# Оценка модели
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1 Score: {f1:.4f}")
cf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

"""**General SVC**"""

from sklearn.svm import SVC
classifier = SVC(kernel='linear', C=0.001, gamma=100,random_state=42)
classifier.fit(X_train, y_train)

y_pred_train = classifier.predict(X_train)
print(X_train)
print(y_pred_train)
y_pred_test = classifier.predict(X_test)
print(X_test)
print(y_pred_test)
accuracy = metrics.accuracy_score(y_test, y_pred_test)
print("Accuracy: {:.2f}".format(accuracy))
# Classification report
print(y_train)
print(y_pred_train)

print(classification_report(y_train, y_pred_train, target_names=class_names))
print("F1 Score: ", f1_score(y_train, y_pred_train, average='micro'))

cf_matrix = confusion_matrix(y_train, y_pred_train)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

"""**GRID Search for SVC**"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, f1_score

param_grid = {
    'kernel': ['linear', 'poly'],
    'gamma': ['scale', 'auto']
}

'''param_grid = {
    'C': np.logspace(-3, 3, 7),
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],
    'degree': [2, 3, 4, 5],
    'coef0': [0.0, 0.1, 0.5, 1.0]
}'''

# SVC
svc = SVC(random_state=42)

# Grid Search Implementation
grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_

print(f"Best params: {grid_search.best_params_}")

# Prediction
y_pred = best_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1 Score: {f1:.4f}")
cf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

"""**Random Forest**"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=2, max_depth=1, min_samples_split=50, min_samples_leaf=20, random_state=42)
classifier.fit(X_train, y_train)

y_pred_train = classifier.predict(X_train)
print(X_train)
print(y_pred_train)
y_pred_test = classifier.predict(X_test)
print(X_test)
print(y_pred_test)
accuracy = metrics.accuracy_score(y_test, y_pred_test)
print("Accuracy: {:.2f}".format(accuracy))
# Classification report
print(y_train)
print(y_pred_train)

print(classification_report(y_train, y_pred_train, target_names=class_names))
print("F1 Score: ", f1_score(y_train, y_pred_train, average='micro'))

cf_matrix = confusion_matrix(y_train, y_pred_train)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

"""**Random Forest Greed Search**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [5, 10],
    'max_depth': [None, 5],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2'],
    'bootstrap': [True, False]
}

'''param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True, False]
}'''

rf = RandomForestClassifier(random_state=42)

# Grid Search Implementation
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_
print(f"Best params: {grid_search.best_params_}")

# Prediction
y_pred = best_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1 Score: {f1:.4f}")
cf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

"""**XGboost**"""

import numpy as np
import pandas as pd
from xgboost import XGBClassifier

classifier = XGBClassifier(n_estimators=5, max_depth=1, learning_rate=0.01, subsample=0.2, colsample_bytree=0.2, reg_lambda=100, random_state=42)
classifier.fit(X_train, y_train)

y_pred_train = classifier.predict(X_train)
print(X_train)
print(y_pred_train)
y_pred_test = classifier.predict(X_test)
print(X_test)
print(y_pred_test)
accuracy = metrics.accuracy_score(y_test, y_pred_test)
print("Accuracy: {:.2f}".format(accuracy))
# Classification report
print(y_train)
print(y_pred_train)

print(classification_report(y_train, y_pred_train, target_names=class_names))
print("F1 Score: ", f1_score(y_train, y_pred_train, average='micro'))

cf_matrix = confusion_matrix(y_train, y_pred_train)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

"""**XGboost Greed Search**"""

import xgboost as xgb
from sklearn.model_selection import GridSearchCV
xgb_model = xgb.XGBRegressor(objective='multi:softprob', eval_metric='logloss', num_class=6)

# Определение параметров для Grid Search
param_grid = {'n_estimators': [1, 3, 5], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5], 'min_child_weight': [1, 3],
              'subsample': [0.7, 0.9], 'colsample_bytree': [0.7, 0.9]}

'''param_grid = {'n_estimators': [100, 300, 500], 'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [3, 5, 7], 'min_child_weight': [1, 3, 5],
              'subsample': [0.7, 0.8, 0.9], 'colsample_bytree': [0.7, 0.8, 0.9]}'''

# Grid Search
grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3,
    verbose=2,
    n_jobs=1
)

grid_search.fit(X_train, y_train)
# Best model
best_model = grid_search.best_estimator_
print(f"Best params: {grid_search.best_params_}")

# Prediction
y_pred = best_model.predict(X_test)



import numpy as np
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

xgb_clf = xgb.XGBClassifier(objective='multi:softmax', num_class=6)

param_grid = {
    'n_estimators': [5, 10],
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'min_child_weight': [1, 5, 10]
}

'''param_grid = {'n_estimators': [100, 300, 500], 'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [3, 5, 7], 'min_child_weight': [1, 3, 5],
              'subsample': [0.7, 0.8, 0.9], 'colsample_bytree': [0.7, 0.8, 0.9]}'''

grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid,
                           scoring='accuracy', cv=3, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_
print(f"Best params: {grid_search.best_params_}")

# Prediction
y_pred = best_model.predict(X_test)

y_pred_train = grid_search.predict(X_train)
print(X_train)
print(y_pred_train)
y_pred_test = grid_search.predict(X_test)
print(X_test)
print(y_pred_test)
accuracy = metrics.accuracy_score(y_test, y_pred_test)
print("Accuracy: {:.2f}".format(accuracy))
# Classification report
print(y_train)
print(y_pred_train)

print(classification_report(y_train, y_pred_train, target_names=class_names))
print("F1 Score: ", f1_score(y_train, y_pred_train, average='micro'))

cf_matrix = confusion_matrix(y_train, y_pred_train)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

"""**Реализация модели CNN**"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Accuracy
from tensorflow.keras.losses import SparseCategoricalCrossentropy

if y.ndim > 1:
    y = np.argmax(y, axis=1)
# Преобразуем метки в числовой формат
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
# Преобразуем метки в one-hot encoding
y = to_categorical(y)
# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
# CNN ожидает, что входные данные имеют форму (samples, time_steps, features)
X_train = np.expand_dims(X_train, axis=2)
X_test = np.expand_dims(X_test, axis=2)
# Построение модели CNN
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.7))
model.add(Dense(y.shape[1], activation='softmax'))  # softmax для многоклассовой классификации
# Компиляция модели
model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])
# Обучение модели
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Оценка модели
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Test Accuracy:", test_accuracy)
# Преобразуем y_test из one-hot encoded в одномерный массив целых чисел
y_test_labels = np.argmax(y_test, axis=1)

# Предсказания модели также конвертируем в метки классов
y_pred = np.argmax(model.predict(X_test), axis=1)

# Печать метрик
print("Test Accuracy:", accuracy_score(y_test_labels, y_pred))
print("Classification Report:\n", classification_report(y_test_labels, y_pred))

cf_matrix = confusion_matrix(y_test_labels, y_pred)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

"""**CNN Grid Search**"""

pip install scikeras

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split, GridSearchCV
from scikeras.wrappers import KerasClassifier
from sklearn.pipeline import make_pipeline

def create_cnn(optimizer='adam', filters=32, kernel_size=3, dense_units=128):
    model = keras.Sequential([
        layers.Conv2D(filters, (kernel_size, kernel_size), activation='relu', input_shape=(128, 128, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(filters * 2, (kernel_size, kernel_size), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(dense_units, activation='relu'),
        layers.Dense(6, activation='softmax')  # 6 классов загрязнения воздуха
    ])

    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(model=create_cnn, optimizer='adam', filters=32, kernel_size=3, dense_units=128, epochs=10, batch_size=32, verbose=0)

pipeline = make_pipeline(model)

param_grid = {
    'kerascclassifier__optimizer': ['adam', 'sgd'],
    'kerascclassifier__filters': [32, 64],
    'kerascclassifier__kernel_size': [3, 5],
    'kerascclassifier__dense_units': [128, 256]
}

'''param_grid = {
    'model__filters': [32, 64, 128],
    'model__kernel_size': [3, 5, 7, 9],
    'model__dense_units': [64, 128, 256],
    'model__dropout_rate': [0.3, 0.5, 0,7, 0,8],
    'batch_size': [32, 64, 128, 256],
    'epochs': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 100]
}
'''

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring={'accuracy': 'accuracy'}, refit='accuracy', cv=3, n_jobs=-1, verbose=1)
#grid_search.fit(X_train, y_train)

if y.ndim > 1:
    y = np.argmax(y, axis=1)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = np.expand_dims(X_train, axis=2)
X_test = np.expand_dims(X_test, axis=2)
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=127, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(y.shape[1], activation='softmax'))  # softmax для многоклассовой классификации
# Компиляция модели
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
# Обучение модели
history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_split=0.2)

# Оценка модели
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Test Accuracy:", test_accuracy)
# Преобразуем y_test из one-hot encoded в одномерный массив целых чисел
y_test_labels = np.argmax(y_test, axis=1)

# Предсказания модели также конвертируем в метки классов
y_pred = np.argmax(model.predict(X_test), axis=1)

# Печать метрик
print("Test Accuracy:", accuracy_score(y_test_labels, y_pred))
print("Classification Report:\n", classification_report(y_test_labels, y_pred))

cf_matrix = confusion_matrix(y_test_labels, y_pred)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

"""**Реализация модели LSTM**"""

import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.losses import SparseCategoricalCrossentropy

sequence_length = 10  # длина последовательности для LSTM
def create_sequences(X, y, seq_length):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_length):
        X_seq.append(X[i:i + seq_length])
        y_seq.append(y[i + seq_length])
    return np.array(X_seq), np.array(y_seq)
X_seq, y_seq = create_sequences(X, y, sequence_length)
# Разделение на обучающий и тестовый наборы
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.75, random_state=42)

# Создание модели LSTM
model = Sequential()
model.add(LSTM(64, input_shape=(sequence_length, X.shape[1]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(32, return_sequences=False))
model.add(Dropout(0.1))
model.add(Dense(32, activation='relu'))
model.add(Dense(6, activation='softmax'))  # Выходной слой для  классификации
#Компиляция модели
model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy', 'sparse_categorical_accuracy'])

#Обучение модели
history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.1)

y_pred = np.argmax(model.predict(X_test), axis=1)
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

cf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)